import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# TensorFlow is causing compatibility issues, so we'll use a simplified approach
TENSORFLOW_AVAILABLE = False

class BatteryMLModels:
    """
    Machine learning models for battery State-of-Health prediction
    """
    
    def __init__(self):
        self.models = {}
        self.model_scores = {}
        self.feature_importance = {}
        self.is_trained = False
        
    def prepare_lstm_data(self, X, y, sequence_length=24):
        """Prepare data for LSTM model"""
        X_lstm = []
        y_lstm = []
        
        for i in range(sequence_length, len(X)):
            X_lstm.append(X.iloc[i-sequence_length:i].values)
            y_lstm.append(y.iloc[i])
        
        return np.array(X_lstm), np.array(y_lstm)
    
    def train_random_forest(self, X_train, X_test, y_train, y_test):
        """Train Random Forest model"""
        print("Training Random Forest model...")
        
        # Hyperparameter tuning
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [10, 20, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        
        rf = RandomForestRegressor(random_state=42, n_jobs=-1)
        
        # Grid search with cross-validation
        grid_search = GridSearchCV(
            rf, param_grid, cv=3, scoring='neg_mean_absolute_error',
            n_jobs=-1, verbose=0
        )
        
        grid_search.fit(X_train, y_train)
        
        # Best model
        best_rf = grid_search.best_estimator_
        
        # Predictions
        y_pred_train = best_rf.predict(X_train)
        y_pred_test = best_rf.predict(X_test)
        
        # Metrics
        train_score = r2_score(y_train, y_pred_train)
        test_score = r2_score(y_test, y_pred_test)
        mae = mean_absolute_error(y_test, y_pred_test)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
        
        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': best_rf.feature_importances_
        }).sort_values('importance', ascending=False)
        
        self.models['Random Forest'] = best_rf
        self.model_scores['Random Forest'] = {
            'train_r2': train_score,
            'test_r2': test_score,
            'mae': mae,
            'rmse': rmse,
            'accuracy_percentage': max(0, (1 - mae/100) * 100)  # Convert MAE to accuracy
        }
        self.feature_importance['Random Forest'] = feature_importance
        
        print(f"‚úì Random Forest - Test R¬≤: {test_score:.4f}, MAE: {mae:.2f}, Accuracy: {self.model_scores['Random Forest']['accuracy_percentage']:.1f}%")
        
        return best_rf, y_pred_test
    
    def train_xgboost(self, X_train, X_test, y_train, y_test):
        """Train XGBoost model"""
        print("Training XGBoost model...")
        
        # Hyperparameter tuning
        param_grid = {
            'n_estimators': [100, 200, 300],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [3, 6, 9],
            'subsample': [0.8, 0.9, 1.0]
        }
        
        xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1)
        
        # Grid search with cross-validation
        grid_search = GridSearchCV(
            xgb_model, param_grid, cv=3, scoring='neg_mean_absolute_error',
            n_jobs=-1, verbose=0
        )
        
        grid_search.fit(X_train, y_train)
        
        # Best model
        best_xgb = grid_search.best_estimator_
        
        # Predictions
        y_pred_train = best_xgb.predict(X_train)
        y_pred_test = best_xgb.predict(X_test)
        
        # Metrics
        train_score = r2_score(y_train, y_pred_train)
        test_score = r2_score(y_test, y_pred_test)
        mae = mean_absolute_error(y_test, y_pred_test)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
        
        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': best_xgb.feature_importances_
        }).sort_values('importance', ascending=False)
        
        self.models['XGBoost'] = best_xgb
        self.model_scores['XGBoost'] = {
            'train_r2': train_score,
            'test_r2': test_score,
            'mae': mae,
            'rmse': rmse,
            'accuracy_percentage': max(0, (1 - mae/100) * 100)
        }
        self.feature_importance['XGBoost'] = feature_importance
        
        print(f"‚úì XGBoost - Test R¬≤: {test_score:.4f}, MAE: {mae:.2f}, Accuracy: {self.model_scores['XGBoost']['accuracy_percentage']:.1f}%")
        
        return best_xgb, y_pred_test
    
    def train_lstm(self, X_train, X_test, y_train, y_test, sequence_length=24):
        """Train LSTM model for time series prediction"""
        if not TENSORFLOW_AVAILABLE:
            print("‚ö†Ô∏è TensorFlow not available. Skipping LSTM training...")
            return None, None
            
        print("Training LSTM model...")
        
        try:
            # Prepare data for LSTM
            X_train_lstm, y_train_lstm = self.prepare_lstm_data(
                X_train.reset_index(drop=True), 
                y_train.reset_index(drop=True), 
                sequence_length
            )
            X_test_lstm, y_test_lstm = self.prepare_lstm_data(
                X_test.reset_index(drop=True), 
                y_test.reset_index(drop=True), 
                sequence_length
            )
            
            # Build LSTM model
            model = Sequential([
                LSTM(50, return_sequences=True, input_shape=(sequence_length, X_train.shape[1])),
                Dropout(0.2),
                LSTM(50, return_sequences=False),
                Dropout(0.2),
                Dense(25),
                Dense(1)
            ])
            
            model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])
            
            # Train model
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
            
            history = model.fit(
                X_train_lstm, y_train_lstm,
                batch_size=32,
                epochs=50,
                validation_data=(X_test_lstm, y_test_lstm),
                callbacks=[early_stopping],
                verbose=0
            )
            
            # Predictions
            y_pred_train = model.predict(X_train_lstm, verbose=0).flatten()
            y_pred_test = model.predict(X_test_lstm, verbose=0).flatten()
            
            # Metrics
            train_score = r2_score(y_train_lstm, y_pred_train)
            test_score = r2_score(y_test_lstm, y_pred_test)
            mae = mean_absolute_error(y_test_lstm, y_pred_test)
            rmse = np.sqrt(mean_squared_error(y_test_lstm, y_pred_test))
            
            self.models['LSTM'] = model
            self.model_scores['LSTM'] = {
                'train_r2': train_score,
                'test_r2': test_score,
                'mae': mae,
                'rmse': rmse,
                'accuracy_percentage': max(0, (1 - mae/100) * 100)
            }
            
            print(f"‚úì LSTM - Test R¬≤: {test_score:.4f}, MAE: {mae:.2f}, Accuracy: {self.model_scores['LSTM']['accuracy_percentage']:.1f}%")
            
            return model, y_pred_test
            
        except Exception as e:
            print(f"‚ö†Ô∏è LSTM training failed: {str(e)}")
            print("Continuing with Random Forest and XGBoost models...")
            return None, None
    
    def train_all_models(self, X, y, test_size=0.2, sequence_length=24):
        """Train all ML models"""
        print("Starting model training pipeline...")
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, shuffle=False
        )
        
        print(f"Training set size: {len(X_train)}")
        print(f"Test set size: {len(X_test)}")
        
        # Train models
        rf_model, rf_pred = self.train_random_forest(X_train, X_test, y_train, y_test)
        xgb_model, xgb_pred = self.train_xgboost(X_train, X_test, y_train, y_test)
        lstm_model, lstm_pred = self.train_lstm(X_train, X_test, y_train, y_test, sequence_length)
        
        self.is_trained = True
        
        # Store test data for visualization
        self.test_data = {
            'X_test': X_test,
            'y_test': y_test,
            'predictions': {
                'Random Forest': rf_pred,
                'XGBoost': xgb_pred
            }
        }
        
        if lstm_pred is not None:
            # Adjust LSTM predictions to match test set size
            lstm_adjusted = np.full(len(y_test), np.nan)
            if len(lstm_pred) > 0:
                lstm_adjusted[-len(lstm_pred):] = lstm_pred
            self.test_data['predictions']['LSTM'] = lstm_adjusted
        
        print("\nüéâ Model training completed!")
        self.print_model_summary()
        
        return self.models
    
    def predict_single_vehicle(self, vehicle_data, model_name='XGBoost'):
        """Make prediction for a single vehicle"""
        if not self.is_trained:
            raise ValueError("Models must be trained before making predictions")
        
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not available")
        
        model = self.models[model_name]
        
        # Ensure data format matches training data
        if isinstance(vehicle_data, pd.Series):
            vehicle_data = vehicle_data.to_frame().T
        
        # Make prediction
        if model_name == 'LSTM':
            # For LSTM, we need sequence data - simplified prediction
            prediction = model.predict(vehicle_data.values.reshape(1, 1, -1), verbose=0)[0][0]
        else:
            prediction = model.predict(vehicle_data)[0]
        
        # Calculate confidence interval (simplified)
        if model_name in ['Random Forest', 'XGBoost']:
            mae = self.model_scores[model_name]['mae']
            confidence_interval = (max(70, prediction - 1.96*mae), 
                                 min(100, prediction + 1.96*mae))
        else:
            confidence_interval = (max(70, prediction - 5), min(100, prediction + 5))
        
        return {
            'prediction': round(prediction, 2),
            'confidence_interval': confidence_interval,
            'model_accuracy': self.model_scores[model_name]['accuracy_percentage']
        }
    
    def get_model_comparison(self):
        """Get comparison of all trained models"""
        if not self.is_trained:
            return pd.DataFrame()
        
        comparison_data = []
        for model_name, scores in self.model_scores.items():
            comparison_data.append({
                'Model': model_name,
                'Test R¬≤': scores['test_r2'],
                'MAE': scores['mae'],
                'RMSE': scores['rmse'],
                'Accuracy (%)': scores['accuracy_percentage'],
                'Target Met (>95%)': '‚úÖ' if scores['accuracy_percentage'] > 95 else '‚ùå'
            })
        
        return pd.DataFrame(comparison_data).sort_values('Accuracy (%)', ascending=False)
    
    def print_model_summary(self):
        """Print summary of model performance"""
        print("\n" + "="*60)
        print("MODEL PERFORMANCE SUMMARY")
        print("="*60)
        
        for model_name, scores in self.model_scores.items():
            accuracy = scores['accuracy_percentage']
            status = "‚úÖ TARGET MET" if accuracy > 95 else "‚ùå Below Target"
            
            print(f"\n{model_name}:")
            print(f"  Accuracy: {accuracy:.1f}% {status}")
            print(f"  R¬≤ Score: {scores['test_r2']:.4f}")
            print(f"  MAE: {scores['mae']:.2f}")
            print(f"  RMSE: {scores['rmse']:.2f}")
        
        # Best model
        best_model = max(self.model_scores.keys(), 
                        key=lambda k: self.model_scores[k]['accuracy_percentage'])
        best_accuracy = self.model_scores[best_model]['accuracy_percentage']
        
        print(f"\nüèÜ BEST MODEL: {best_model} ({best_accuracy:.1f}% accuracy)")
        print("="*60)
    
    def get_feature_importance_top_n(self, model_name, n=10):
        """Get top N important features for a model"""
        if model_name not in self.feature_importance:
            return pd.DataFrame()
        
        return self.feature_importance[model_name].head(n)
    
    def predict_degradation_trend(self, vehicle_data, days_ahead=30, model_name='XGBoost'):
        """Predict degradation trend for specified days ahead"""
        if not self.is_trained:
            raise ValueError("Models must be trained before making predictions")
        
        current_soh = self.predict_single_vehicle(vehicle_data, model_name)['prediction']
        
        # Simplified trend prediction based on historical degradation patterns
        daily_degradation_rate = 0.01  # Approximate daily degradation
        
        trend_predictions = []
        for day in range(1, days_ahead + 1):
            predicted_soh = max(70, current_soh - (daily_degradation_rate * day))
            trend_predictions.append({
                'days_ahead': day,
                'predicted_soh': round(predicted_soh, 2)
            })
        
        return pd.DataFrame(trend_predictions)
